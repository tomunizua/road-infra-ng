{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e925a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tdngo\\road-infra-ng\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing opencv-python...\n",
      "Installing scikit-learn...\n",
      "All libraries imported and packages checked.\n",
      "PyTorch version: 2.9.0+cpu, CUDA available: False\n",
      "\n",
      "Analyzing laptop capabilities (Conservative Settings)...\n",
      "CPU: 4 logical cores\n",
      "RAM: 15.8GB\n",
      "No powerful GPU detected - **CPU or Low-VRAM training**.\n",
      "\n",
      "Optimized settings for stability:\n",
      "  Batch Size: 2\n",
      "  Image Size: 416\n",
      "  Workers: 2\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# SECTION 1: PACKAGE INSTALLATION AND SETUP\n",
    "# =========================================================================\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "import json\n",
    "import psutil\n",
    "from ultralytics import YOLO\n",
    "import optuna\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Define the source and target directories relative to the notebook location\n",
    "# Assuming your previously combined (80k) dataset is locally available here:\n",
    "SOURCE_DIR = Path(\"combined_balanced_dataset\") \n",
    "\n",
    "# Target for the new, smaller 24k-image dataset\n",
    "TARGET_DIR = Path(\"rdd_sampled_24k_local\")\n",
    "\n",
    "# Target training set size (excluding validation split)\n",
    "TARGET_TRAIN_SIZE = 24000 \n",
    "\n",
    "# Define the classes (must match your dataset.yaml)\n",
    "CLASS_NAMES = {\n",
    "    0: 'longitudinal crack',\n",
    "    1: 'transverse crack',\n",
    "    2: 'alligator crack',\n",
    "    3: 'other corruption',\n",
    "    4: 'Pothole'\n",
    "}\n",
    "\n",
    "# Install necessary packages if missing (laptop-specific path handling not needed here)\n",
    "def install_packages():\n",
    "    packages = [\"ultralytics\", \"optuna\", \"kaggle\", \"opencv-python\", \"albumentations\", \"psutil\", \"scikit-learn\"]\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "install_packages()\n",
    "\n",
    "print(\"All libraries imported and packages checked.\")\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# =========================================================================\n",
    "# SECTION 2: RESOURCE MONITOR AND CONSERVATIVE OPTIMIZATION (FOR LAPTOP)\n",
    "# =========================================================================\n",
    "\n",
    "class LaptopResourceMonitor:\n",
    "    \"\"\"Monitor and manage laptop resources during training\"\"\"\n",
    "    \n",
    "    def __init__(self, max_training_hours=8): # Increased max hours for a CPU/slower-GPU laptop run\n",
    "        self.max_training_hours = max_training_hours\n",
    "        self.start_time = time.time()\n",
    "        self.gpu_available = torch.cuda.is_available()\n",
    "        self.initial_setup()\n",
    "    \n",
    "    def initial_setup(self):\n",
    "        print(\"\\nAnalyzing laptop capabilities (Conservative Settings)...\")\n",
    "        cpu_count = psutil.cpu_count(logical=False) or psutil.cpu_count()\n",
    "        memory = psutil.virtual_memory()\n",
    "        \n",
    "        print(f\"CPU: {cpu_count} logical cores\")\n",
    "        print(f\"RAM: {memory.total / (1024**3):.1f}GB\")\n",
    "        \n",
    "        if self.gpu_available and torch.cuda.get_device_properties(0).total_memory > 2 * (1024**3):\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}, Memory: {gpu_memory:.1f}GB\")\n",
    "            \n",
    "            # Moderate GPU settings\n",
    "            self.batch_size = 8\n",
    "            self.image_size = 640\n",
    "            print(\"Moderate GPU detected - using balanced settings.\")\n",
    "        else:\n",
    "            print(\"No powerful GPU detected - **CPU or Low-VRAM training**.\")\n",
    "            # Highly conservative settings for stability\n",
    "            self.batch_size = 2 # VERY small batch for VRAM/RAM stability\n",
    "            self.image_size = 416\n",
    "        \n",
    "        self.workers = min(cpu_count // 2, 4) # Max 4 workers for CPU bound tasks\n",
    "        \n",
    "        print(f\"\\nOptimized settings for stability:\")\n",
    "        print(f\"  Batch Size: {self.batch_size}\")\n",
    "        print(f\"  Image Size: {self.image_size}\")\n",
    "        print(f\"  Workers: {self.workers}\")\n",
    "    \n",
    "    def get_optimized_config(self):\n",
    "        return {\n",
    "            'batch_size': self.batch_size,\n",
    "            'image_size': self.image_size,\n",
    "            'workers': self.workers,\n",
    "            'epochs': min(100, int(self.max_training_hours * 8)), # Max 8 epochs per hour estimate\n",
    "            'patience': 20,\n",
    "            'amp': self.gpu_available, # Only use mixed precision if GPU is available\n",
    "            'cache': False, # Keep off for large dataset on HDD/SSD to save RAM\n",
    "            'save_period': -1,\n",
    "        }\n",
    "\n",
    "monitor = LaptopResourceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6fcf19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING CLASS-BALANCED SAMPLING...\n",
      "Analyzing current training labels for class distribution...\n",
      "Analysis complete.\n",
      "Images per class (Train Source): {'longitudinal crack': 9457, 'transverse crack': 5433, 'alligator crack': 5976, 'other corruption': 5247, 'Pothole': 8896}\n",
      "Final sampled training image count: 24000\n",
      "Creating new dataset structure and copying files...\n",
      "Copying 24000 training files...\n",
      "Copying 7333 validation files...\n",
      "Copying complete.\n",
      "\n",
      " Sampling SUCCESS! \n",
      "New dataset location: rdd_sampled_24k_local\n",
      "New YAML file: rdd_sampled_24k_local\\dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# SECTION 3: CLASS-BALANCED SAMPLING (24K IMAGES)\n",
    "# =========================================================================\n",
    "\n",
    "class ClassBalancedSampler:\n",
    "    \"\"\"Samples images to meet a target count while preserving class distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, target_dir, target_train_size):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.target_train_size = target_train_size\n",
    "        self.class_images = {}\n",
    "        # Assuming the source structure is 'images/train' and 'labels/train'\n",
    "        self.val_images = list((self.source_dir / 'images' / 'val').glob('*'))\n",
    "        self.train_images = list((self.source_dir / 'images' / 'train').glob('*'))\n",
    "\n",
    "    def _analyze_labels(self):\n",
    "        print(\"Analyzing current training labels for class distribution...\")\n",
    "        \n",
    "        for class_id in CLASS_NAMES.keys():\n",
    "            self.class_images[class_id] = []\n",
    "            \n",
    "        for img_path in self.train_images:\n",
    "            label_path = self.source_dir / 'labels' / 'train' / (img_path.stem + '.txt')\n",
    "            \n",
    "            if label_path.exists():\n",
    "                try:\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        content = f.read().strip()\n",
    "                        if content:\n",
    "                            # Use set comprehension for speed and correct parsing\n",
    "                            present_classes = set(int(line.split()[0]) \n",
    "                                                  for line in content.split('\\n') if line.strip())\n",
    "                            \n",
    "                            for class_id in present_classes:\n",
    "                                self.class_images[class_id].append(img_path)\n",
    "                except: continue\n",
    "        \n",
    "        print(\"Analysis complete.\")\n",
    "        current_counts = {CLASS_NAMES[k]: len(v) for k, v in self.class_images.items()}\n",
    "        print(f\"Images per class (Train Source): {current_counts}\")\n",
    "        return current_counts\n",
    "\n",
    "    def _select_sample_images(self, current_counts):\n",
    "        \"\"\"Selects a unique list of images while maintaining class balance.\"\"\"\n",
    "        \n",
    "        total_images_in_source = len(self.train_images)\n",
    "        if total_images_in_source == 0:\n",
    "            print(\"ERROR: No images found in source training folder.\")\n",
    "            return []\n",
    "            \n",
    "        # Target number of images for each class based on the smallest class, scaled up\n",
    "        # We ensure every class gets at least 'target_per_class' samples (images that contain that class)\n",
    "        \n",
    "        # Calculate proportional target per class based on current distribution\n",
    "        total_annotations = sum(c['count'] for c in monitor_combiner.rdd_distribution.values() if 'count' in c)\n",
    "        \n",
    "        # Simple proportionate sampling based on current distribution (more robust than min-count)\n",
    "        # We use the existing class imbalance knowledge to guide sampling.\n",
    "        selected_images = set()\n",
    "        \n",
    "        for class_id, count_dict in monitor_combiner.rdd_distribution.items():\n",
    "            if 'count' not in count_dict: continue\n",
    "                \n",
    "            class_annotations = count_dict['count']\n",
    "            \n",
    "            # Sample image count proportional to its annotation count relative to the whole dataset\n",
    "            if total_annotations > 0:\n",
    "                proportion = class_annotations / total_annotations\n",
    "            else:\n",
    "                proportion = 1 / len(CLASS_NAMES) # Equal weight if no annotations found\n",
    "                \n",
    "            # Max number of images to try to select for this class based on overall target\n",
    "            target_image_count_for_class = int(self.target_train_size * proportion * 1.5) # 1.5 multiplier for overlap\n",
    "\n",
    "            img_list = self.class_images.get(class_id, [])\n",
    "            random.shuffle(img_list)\n",
    "            \n",
    "            count = 0\n",
    "            for img_path in img_list:\n",
    "                if img_path not in selected_images and count < target_image_count_for_class:\n",
    "                    selected_images.add(img_path)\n",
    "                    count += 1\n",
    "                \n",
    "                if len(selected_images) >= self.target_train_size:\n",
    "                    break\n",
    "            \n",
    "            if len(selected_images) >= self.target_train_size:\n",
    "                break\n",
    "        \n",
    "        # Fill up remaining if the target wasn't met through class-specific sampling\n",
    "        remaining_to_sample = self.target_train_size - len(selected_images)\n",
    "        if remaining_to_sample > 0:\n",
    "            print(f\"Need {remaining_to_sample} more images to hit target. Filling randomly.\")\n",
    "            unselected_images = list(set(self.train_images) - selected_images)\n",
    "            random.shuffle(unselected_images)\n",
    "            selected_images.update(unselected_images[:remaining_to_sample])\n",
    "\n",
    "        print(f\"Final sampled training image count: {len(selected_images)}\")\n",
    "        return list(selected_images)\n",
    "\n",
    "\n",
    "    def _copy_dataset(self, selected_images):\n",
    "        \"\"\"Creates the new, smaller dataset structure.\"\"\"\n",
    "        print(\"Creating new dataset structure and copying files...\")\n",
    "        \n",
    "        if self.target_dir.exists():\n",
    "             shutil.rmtree(self.target_dir) # Remove old directory\n",
    "        \n",
    "        for split in ['train', 'val']:\n",
    "            (self.target_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "            (self.target_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        # Copy selected training data\n",
    "        print(f\"Copying {len(selected_images)} training files...\")\n",
    "        for img_path in selected_images:\n",
    "            shutil.copy2(img_path, self.target_dir / 'images' / 'train' / img_path.name)\n",
    "            label_path = self.source_dir / 'labels' / 'train' / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                shutil.copy2(label_path, self.target_dir / 'labels' / 'train' / (img_path.stem + '.txt'))\n",
    "        \n",
    "        # Copy *all* validation data (to keep test consistent)\n",
    "        print(f\"Copying {len(self.val_images)} validation files...\")\n",
    "        for img_path in self.val_images:\n",
    "            shutil.copy2(img_path, self.target_dir / 'images' / 'val' / img_path.name)\n",
    "            label_path = self.source_dir / 'labels' / 'val' / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                shutil.copy2(label_path, self.target_dir / 'labels' / 'val' / (img_path.stem + '.txt'))\n",
    "                \n",
    "        print(\"Copying complete.\")\n",
    "\n",
    "    def _create_yaml(self):\n",
    "        \"\"\"Creates the new dataset YAML file.\"\"\"\n",
    "        yaml_config = {\n",
    "            'path': str(self.target_dir.absolute()), \n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'nc': len(CLASS_NAMES),\n",
    "            'names': CLASS_NAMES\n",
    "        }\n",
    "        \n",
    "        yaml_file = self.target_dir / 'dataset.yaml'\n",
    "        with open(yaml_file, 'w') as f:\n",
    "            yaml.dump(yaml_config, f, sort_keys=False)\n",
    "            \n",
    "        return str(yaml_file)\n",
    "\n",
    "    def run_sampling(self):\n",
    "        \"\"\"Executes the full sampling pipeline.\"\"\"\n",
    "        current_counts = self._analyze_labels()\n",
    "        selected_images = self._select_sample_images(current_counts)\n",
    "        self._copy_dataset(selected_images)\n",
    "        new_yaml_file = self._create_yaml()\n",
    "        \n",
    "        print(\"\\n Sampling SUCCESS! \")\n",
    "        print(f\"New dataset location: {self.target_dir}\")\n",
    "        print(f\"New YAML file: {new_yaml_file}\")\n",
    "        \n",
    "        return new_yaml_file\n",
    "\n",
    "# --- HACK: Re-using the class distribution analysis from the COMBINER for better proportional sampling ---\n",
    "# NOTE: This assumes you have the distribution knowledge from your previous run.\n",
    "# If you don't have this, you'll need to re-run the full data analysis step first.\n",
    "\n",
    "class DummyCombinerMonitor:\n",
    "    \"\"\"Simulates the distribution analysis result to enable proportional sampling.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Using the distribution you saw in the Colab output (approximate annotations count)\n",
    "        self.rdd_distribution = {\n",
    "            0: {'count': 22091}, # longitudinal crack (high)\n",
    "            1: {'count': 10155}, # transverse crack (moderate)\n",
    "            2: {'count': 9080},  # alligator crack (moderate)\n",
    "            3: {'count': 9118},  # other corruption (moderate)\n",
    "            4: {'count': 27429}, # Pothole (high)\n",
    "        }\n",
    "monitor_combiner = DummyCombinerMonitor()\n",
    "\n",
    "# --- EXECUTE THE SAMPLING ---\n",
    "print(\"STARTING CLASS-BALANCED SAMPLING...\")\n",
    "sampler = ClassBalancedSampler(SOURCE_DIR, TARGET_DIR, TARGET_TRAIN_SIZE)\n",
    "yaml_file = sampler.run_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcdcbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 14:44:36,772] A new study created in memory with name: no-name-c556a685-6cf7-40b4-a21e-cb61199d6e8a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting FAST Bayesian hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.227 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.221  Python-3.10.0 torch-2.9.0+cpu CPU (Intel Core i7-8705G 3.10GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=2, bgr=0.0, box=0.05508430131440244, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.4358569455220789, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=rdd_sampled_24k_local\\dataset.yaml, degrees=1.2894838326758262, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.017255863620831164, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=trial_0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=optuna_trials_local, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=C:\\Users\\tdngo\\road-infra-ng\\notebooks\\optuna_trials_local\\trial_0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.3708657319520028, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=6, warmup_momentum=0.8, weight_decay=0.0005, workers=1, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,137,535 parameters, 11,137,519 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 24.539.6 MB/s, size: 309.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\labels\\train... 24000 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 24000/24000 276.4it/s 1:27<0.0ss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\images\\train\\Japan_006916.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\images\\train\\Japan_011427.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\labels\\train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 24.734.1 MB/s, size: 197.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\labels\\val... 7333 images, 1837 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 7333/7333 270.4it/s 27.1s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mC:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\images\\val\\Japan_006536.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\tdngo\\road-infra-ng\\notebooks\\rdd_sampled_24k_local\\labels\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.017255863620831164' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\tdngo\\road-infra-ng\\notebooks\\optuna_trials_local\\trial_0\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/5         0G    0.01559       2.48      1.829          4        416: 100% ━━━━━━━━━━━━ 12000/12000 1.0it/s 3:17:23<1.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 1.6it/s 19:15<0.6s\n",
      "                   all       7333      14016      0.236      0.325      0.198      0.079\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/5         0G    0.01496      2.128      1.777          4        416: 100% ━━━━━━━━━━━━ 12000/12000 1.1it/s 3:00:49<0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 1.5it/s 19:50<1.0s\n",
      "                   all       7333      14016      0.391      0.352      0.299      0.131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/5         0G    0.01451      1.995      1.741         10        416: 100% ━━━━━━━━━━━━ 12000/12000 1.1it/s 3:06:51<0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 1.3it/s 23:07<0.8s\n",
      "                   all       7333      14016      0.401      0.392      0.318      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        4/5         0G    0.01409      1.881      1.694          3        416: 100% ━━━━━━━━━━━━ 12000/12000 0.9it/s 3:44:32<1.2ss1\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 1.4it/s 22:15<0.8s\n",
      "                   all       7333      14016      0.464       0.42      0.353       0.16\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        5/5         0G    0.01359       1.76      1.644          4        416: 100% ━━━━━━━━━━━━ 12000/12000 0.9it/s 3:42:22<1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 1.4it/s 22:06<0.6s\n",
      "                   all       7333      14016      0.467      0.457       0.38      0.177\n",
      "\n",
      "5 epochs completed in 18.644 hours.\n",
      "Optimizer stripped from C:\\Users\\tdngo\\road-infra-ng\\notebooks\\optuna_trials_local\\trial_0\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from C:\\Users\\tdngo\\road-infra-ng\\notebooks\\optuna_trials_local\\trial_0\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating C:\\Users\\tdngo\\road-infra-ng\\notebooks\\optuna_trials_local\\trial_0\\weights\\best.pt...\n",
      "Ultralytics 8.3.221  Python-3.10.0 torch-2.9.0+cpu CPU (Intel Core i7-8705G 3.10GHz)\n",
      "Model summary (fused): 72 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1834/1834 2.0it/s 15:36<0.4s\n",
      "                   all       7333      14016      0.467      0.457       0.38      0.177\n",
      "Speed: 0.9ms preprocess, 108.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.380245:  12%|█▎        | 1/8 [18:56:24<132:34:49, 68184.26s/it, 68184.26/900 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 09:41:01,037] Trial 0 finished with value: 0.380245434046406 and parameters: {'lr0': 0.017255863620831164, 'box': 0.05508430131440244, 'cls': 0.4358569455220789, 'warmup_epochs': 6, 'degrees': 1.2894838326758262, 'scale': 0.3708657319520028}. Best is trial 0 with value: 0.380245434046406.\n",
      "\n",
      "Optimization complete! Best mAP@0.5: 0.380\n",
      "Best parameters: {'lr0': 0.017255863620831164, 'box': 0.05508430131440244, 'cls': 0.4358569455220789, 'warmup_epochs': 6, 'degrees': 1.2894838326758262, 'scale': 0.3708657319520028}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# SECTION 4: BAYESIAN HYPERPARAMETER OPTIMIZATION (FAST)\n",
    "# =========================================================================\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    \"\"\"Efficient Bayesian optimization for laptop training\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_yaml, resource_monitor):\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.monitor = resource_monitor\n",
    "        self.best_params = {}\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optimization objective function (short 5 epoch trial)\"\"\"\n",
    "        \n",
    "        # Suggest hyperparameters (using corrected names)\n",
    "        lr0 = trial.suggest_float('lr0', 0.005, 0.02)\n",
    "        box = trial.suggest_float('box', 0.02, 0.08) # CORRECTED NAME\n",
    "        cls = trial.suggest_float('cls', 0.3, 0.7)   # CORRECTED NAME\n",
    "        warmup_epochs = trial.suggest_int('warmup_epochs', 3, 8)\n",
    "        degrees = trial.suggest_float('degrees', 0, 10)\n",
    "        scale = trial.suggest_float('scale', 0.1, 0.4)\n",
    "        \n",
    "        try:\n",
    "            model = YOLO('yolov8s.pt')\n",
    "            config = self.monitor.get_optimized_config()\n",
    "            \n",
    "            results = model.train(\n",
    "                data=self.dataset_yaml,\n",
    "                epochs=5,  # VERY short run for trials on laptop\n",
    "                batch=config['batch_size'],\n",
    "                imgsz=config['image_size'],\n",
    "                lr0=lr0,\n",
    "                box=box, # CORRECTED\n",
    "                cls=cls, # CORRECTED\n",
    "                warmup_epochs=warmup_epochs,\n",
    "                degrees=degrees,\n",
    "                scale=scale,\n",
    "                patience=3,\n",
    "                workers=min(config['workers'], 1), # Max 1 worker for stable Optuna trials\n",
    "                amp=config['amp'],\n",
    "                cache=False,\n",
    "                verbose=False, plots=False,\n",
    "                project=\"optuna_trials_local\", name=f\"trial_{trial.number}\",\n",
    "                save=False, exist_ok=True\n",
    "            )\n",
    "            \n",
    "            # Use mAP50 for speed\n",
    "            mAP = results.results_dict.get('metrics/mAP50(B)', 0) \n",
    "            return mAP\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def optimize(self, n_trials=8, timeout_minutes=15): # Max 8 trials, 15 min total\n",
    "        print(\"\\nStarting FAST Bayesian hyperparameter optimization...\")\n",
    "        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=2))\n",
    "        \n",
    "        try:\n",
    "            study.optimize(self.objective, n_trials=n_trials, timeout=timeout_minutes * 60, show_progress_bar=True)\n",
    "            self.best_params = study.best_params\n",
    "            \n",
    "            print(f\"\\nOptimization complete! Best mAP@0.5: {study.best_value:.3f}\")\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "            return self.best_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Optimization interrupted: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Execute optimization\n",
    "best_params = {}\n",
    "if 'yaml_file' in locals() and Path(yaml_file).exists():\n",
    "    optimizer = BayesianOptimizer(yaml_file, monitor)\n",
    "    best_params = optimizer.optimize() \n",
    "else:\n",
    "    print(\"Skipping optimization - sampled dataset not ready.\")\n",
    "\n",
    "# Ensure we have defaults if optimization failed\n",
    "if not best_params:\n",
    "    print(\"Using conservative defaults for main training.\")\n",
    "    best_params = {'lr0': 0.01, 'box': 0.05, 'cls': 0.5, 'warmup_epochs': 5, 'degrees': 10, 'scale': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ee8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: Training YAML file not found. Pipeline stopped.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# SECTION 5: OPTIMIZED TRAINING AND EVALUATION\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "class OptimizedTrainer:\n",
    "    \"\"\"Optimized model training with resource management\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_yaml, resource_monitor, best_params):\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.monitor = resource_monitor\n",
    "        self.best_params = best_params\n",
    "        self.model = None\n",
    "        self.results = None\n",
    "        self.run_name = f'rdd_40epoch_laptop_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        # Overriding the max epochs for a faster run plan\n",
    "        self.MAX_EPOCHS = 40 \n",
    "        self.PATIENCE = 10 # Aggressive early stopping\n",
    "        self.val_images = list((Path(self.dataset_yaml).parent / 'images' / 'val').glob('*'))\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        print(f\"\\nTraining optimized YOLOv8 model ({self.MAX_EPOCHS} epochs max)...\")\n",
    "        \n",
    "        model_size = 'yolov8s'\n",
    "        self.model = YOLO(f'{model_size}.pt')\n",
    "        config = self.monitor.get_optimized_config()\n",
    "        \n",
    "        train_params = {\n",
    "            'data': self.dataset_yaml,\n",
    "            'epochs': self.MAX_EPOCHS,\n",
    "            'batch': config['batch_size'],\n",
    "            'imgsz': config['image_size'],\n",
    "            \n",
    "            # Optimized hyperparameters\n",
    "            'lr0': self.best_params.get('lr0', 0.01),\n",
    "            'box': self.best_params.get('box', 0.05),\n",
    "            'cls': self.best_params.get('cls', 0.5),\n",
    "            'warmup_epochs': self.best_params.get('warmup_epochs', 5),\n",
    "            'degrees': self.best_params.get('degrees', 10),\n",
    "            'scale': self.best_params.get('scale', 0.3),\n",
    "            \n",
    "            # Training efficiency & Stability\n",
    "            'patience': self.PATIENCE, # Use faster patience\n",
    "            'workers': config['workers'],\n",
    "            'amp': config['amp'],\n",
    "            'cache': config['cache'],\n",
    "            'save_period': config['save_period'],\n",
    "            'cos_lr': True,\n",
    "            \n",
    "            # Augmentation \n",
    "            'fliplr': 0.5, 'mosaic': 0.8, 'mixup': 0.1,\n",
    "            \n",
    "            'project': 'runs/detect_final_local',\n",
    "            'name': self.run_name,\n",
    "            'plots': True, 'verbose': True, 'exist_ok': True\n",
    "        }\n",
    "        \n",
    "        print(f\"Training parameters: Batch={config['batch_size']}, Workers={config['workers']}, GPU/AMP={config['amp']}\")\n",
    "        \n",
    "        try:\n",
    "            self.results = self.model.train(**train_params)\n",
    "            print(\"\\n Training completed successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\" Training failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Comprehensive model evaluation on the final model\"\"\"\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"==================================================\")\n",
    "\n",
    "        if not self.results:\n",
    "            print(\"No training results available.\")\n",
    "            return None\n",
    "\n",
    "        # Rerunning validation to get clean metrics object\n",
    "        val_results = self.model.val(data=self.dataset_yaml, verbose=True)\n",
    "        \n",
    "        # --- Core Performance Metrics ---\n",
    "        performance = {\n",
    "            'mAP50': val_results.box.map50,\n",
    "            'mAP50_95': val_results.box.map,\n",
    "            'precision': val_results.box.mp,\n",
    "            'recall': val_results.box.mr,\n",
    "            'epochs_target': self.MAX_EPOCHS,\n",
    "            'epochs_trained': val_results.box.map.size, # Use AP array size as proxy for classes/epochs count\n",
    "        }\n",
    "        \n",
    "        print(f\"Performance Metrics (Validation Set):\")\n",
    "        print(f\"  mAP@0.5:    {performance['mAP50']:.3f} ({performance['mAP50']:.1%})\")\n",
    "        print(f\"  mAP@0.5:0.95: {performance['mAP50_95']:.3f} ({performance['mAP50_95']:.1%})\")\n",
    "        print(f\"  Precision:  {performance['precision']:.3f} ({performance['precision']:.1%})\")\n",
    "        print(f\"  Recall:     {performance['recall']:.3f} ({performance['recall']:.1%})\")\n",
    "        \n",
    "        # --- Per-Class Performance ---\n",
    "        print(\"\\nPer-Class Performance (mAP@0.5):\")\n",
    "        class_results = val_results.box.ap_class_index\n",
    "        class_maps = val_results.box.ap\n",
    "        \n",
    "        for i, class_index in enumerate(class_results):\n",
    "            class_name = CLASS_NAMES.get(class_index, f'Class {class_index}')\n",
    "            print(f\"  - {class_name:<20}: {class_maps[i]:.3f}\")\n",
    "            \n",
    "        # --- Improvement Analysis ---\n",
    "        baseline_mAP = 0.417 \n",
    "        improvement = performance['mAP50'] - baseline_mAP\n",
    "        print(f\"\\nImprovement vs Baseline (41.7%): {improvement:+.1%}\")\n",
    "        \n",
    "        return performance\n",
    "\n",
    "# --- EXECUTE FINAL TRAINING AND EVALUATION ---\n",
    "final_performance = None\n",
    "if 'yaml_file' in locals() and Path(yaml_file).exists():\n",
    "    trainer = OptimizedTrainer(yaml_file, monitor, best_params)\n",
    "    \n",
    "    if trainer.train_model():\n",
    "        final_performance = trainer.evaluate_model()\n",
    "\n",
    "    # Save summary to file\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset_size': f\"{trainer.MAX_EPOCHS} train + {len(trainer.val_images)} val\",\n",
    "        'performance': final_performance,\n",
    "        'hyperparameters': best_params,\n",
    "        'model_path': f\"runs/detect_final_local/{trainer.run_name}/weights/best.pt\"\n",
    "    }\n",
    "    with open('laptop_training_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"\\nTraining summary saved to: laptop_training_summary.json\")\n",
    "    print(f\"Best model saved to: {summary['model_path']}\")\n",
    "else:\n",
    "    print(\"FATAL ERROR: Training YAML file not found. Pipeline stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
