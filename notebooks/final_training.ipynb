{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAH2wingnasG",
        "outputId": "6ca7dd14-be54-4022-d7cd-aa472171346f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.228)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (7.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Packages installed and libraries imported.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "‚úÖ Base 2K Dataset found: /content/drive/MyDrive/Colab Notebooks/rdd_sampled_2k_test\n",
            "\n",
            "\n",
            "==================================================\n",
            "‚úÖ SUCCESS: GPU is active!\n",
            "Mon Nov 17 14:53:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Installation and Imports ---\n",
        "!pip install -U ultralytics optuna psutil scikit-learn kaggle\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import yaml\n",
        "import time\n",
        "import numpy as np\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from ultralytics import YOLO\n",
        "import optuna\n",
        "\n",
        "# Import the Colab file downloader utility\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Packages installed and libraries imported.\")\n",
        "\n",
        "# --- 2. Mount Google Drive (to read the dataset) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 3. Define Paths ---\n",
        "# This is the path to your 2K sampled dataset on Google Drive\n",
        "SOURCE_2K_DATASET_PATH = Path('/content/drive/MyDrive/Colab Notebooks/rdd_sampled_2k_test')\n",
        "# ‚≠ê FIX: Define as a Path object to prevent AttributeError\n",
        "BOOSTED_DATASET_PATH = Path('/content/drive/MyDrive/RDD_2K_Pothole_Boosted')\n",
        "# This is the NEW yaml file we will use for training\n",
        "yaml_file = str(BOOSTED_DATASET_PATH / 'boosted_dataset.yaml') # Use the '/' operator\n",
        "\n",
        "if SOURCE_2K_DATASET_PATH.exists():\n",
        "    print(f\"\\n‚úÖ Base 2K Dataset found: {SOURCE_2K_DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERROR: Base 2K Dataset not found at {SOURCE_2K_DATASET_PATH}.\")\n",
        "\n",
        "# --- 4. GPU CHECK ---\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"‚ùå WARNING: NO GPU DETECTED!\")\n",
        "    print(\"Please go to `Runtime` -> `Change runtime type` and select `T4 GPU`.\")\n",
        "    print(\"=\"*50 + \"\\n\\n\")\n",
        "else:\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ SUCCESS: GPU is active!\")\n",
        "    !nvidia-smi\n",
        "    print(\"=\"*50 + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjdTRkgpnu4m",
        "outputId": "ad702da3-b490-4677-d942-1560573848c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to create 'Pothole-Boosted' dataset...\n",
            "Downloading Kaggle dataset...\n",
            "Dataset URL: https://www.kaggle.com/datasets/ryukijanoramunae/pothole-dataset\n",
            "License(s): DbCL-1.0\n",
            "Downloading pothole-dataset.zip to /content/pothole_data_raw\n",
            " 96% 747M/776M [00:04<00:00, 49.6MB/s]\n",
            "100% 776M/776M [00:04<00:00, 198MB/s] \n",
            "Kaggle dataset downloaded successfully.\n",
            "Removing old boosted dataset...\n",
            "Creating new dataset at /content/drive/MyDrive/RDD_2K_Pothole_Boosted...\n",
            "Copied 2K base dataset.\n",
            "Found 2000 new pothole images to merge.\n",
            "Merging 'train' split...\n",
            "Merged 1600 new train images with 3915 pothole annotations.\n",
            "Merging 'val' split...\n",
            "Merged 400 new val images with 955 pothole annotations.\n",
            "\n",
            "‚úÖ 'Pothole-Boosted' 5-Class dataset created.\n",
            "The pipeline will now use this new dataset: /content/drive/MyDrive/RDD_2K_Pothole_Boosted/dataset.yaml\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 2: Create Pothole-Boosted Dataset ---\n",
        "\n",
        "print(\"Starting to create 'Pothole-Boosted' dataset...\")\n",
        "\n",
        "# --- 1. Define Pothole Dataset and Config ---\n",
        "POTHOLE_KAGGLE_SLUG = 'ryukijanoramunae/pothole-dataset'\n",
        "POTHOLE_DOWNLOAD_PATH = Path('/content/pothole_data_raw')\n",
        "POTHOLE_CLASS_ID_ORIGINAL = 0 # In the Kaggle set, potholes are class 0\n",
        "POTHOLE_CLASS_ID_TARGET = 4 # In your 5-class model, potholes are class 4\n",
        "\n",
        "# --- 2. Download Pothole Data from Kaggle ---\n",
        "# ‚≠ê FIX: This 'try' block will now stop the cell if it fails.\n",
        "try:\n",
        "    print(\"Downloading Kaggle dataset...\")\n",
        "    # This command will now succeed because you ran the cell in STEP 2\n",
        "    !kaggle datasets download -d {POTHOLE_KAGGLE_SLUG} -p {POTHOLE_DOWNLOAD_PATH} --unzip\n",
        "    print(\"Kaggle dataset downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FATAL ERROR: Kaggle download failed.\")\n",
        "    print(\"Please ensure your 'kaggle.json' is uploaded and you ran the 'mv' command cell.\")\n",
        "    # Stop execution if download fails\n",
        "    raise e\n",
        "\n",
        "# --- 3. Re-create Target Directory ---\n",
        "if BOOSTED_DATASET_PATH.exists():\n",
        "    print(f\"Removing old boosted dataset...\")\n",
        "    shutil.rmtree(BOOSTED_DATASET_PATH)\n",
        "\n",
        "print(f\"Creating new dataset at {BOOSTED_DATASET_PATH}...\")\n",
        "# Copy the entire 2K dataset as our base\n",
        "shutil.copytree(SOURCE_2K_DATASET_PATH, BOOSTED_DATASET_PATH)\n",
        "print(\"Copied 2K base dataset.\")\n",
        "\n",
        "# --- 4. Find and Merge New Pothole Data ---\n",
        "pothole_images = list(POTHOLE_DOWNLOAD_PATH.rglob('*.jpg'))\n",
        "random.shuffle(pothole_images)\n",
        "\n",
        "MAX_BOOST_IMAGES = 2000\n",
        "pothole_images = pothole_images[:MAX_BOOST_IMAGES]\n",
        "print(f\"Found {len(pothole_images)} new pothole images to merge.\")\n",
        "\n",
        "# Split them 80/20 for train/val\n",
        "train_split_index = int(len(pothole_images) * 0.8)\n",
        "new_train_images = pothole_images[:train_split_index]\n",
        "new_val_images = pothole_images[train_split_index:]\n",
        "\n",
        "# --- 5. Processing Function to Merge and Re-map ---\n",
        "def merge_and_remap(image_list, split):\n",
        "    target_img_dir = BOOSTED_DATASET_PATH / 'images' / split\n",
        "    target_label_dir = BOOSTED_DATASET_PATH / 'labels' / split\n",
        "    images_merged = 0\n",
        "    annos_merged = 0\n",
        "\n",
        "    for i, img_path in enumerate(image_list):\n",
        "        try:\n",
        "            # Find the original label file\n",
        "            label_path = img_path.parent.parent / 'labels' / (img_path.stem + '.txt')\n",
        "            if not label_path.exists():\n",
        "                label_path = img_path.parent / (img_path.stem + '.txt') # Check alternative path\n",
        "                if not label_path.exists():\n",
        "                    continue # Skip if no label\n",
        "\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            new_label_content = []\n",
        "            for line in lines:\n",
        "                parts = line.split()\n",
        "                if not parts: continue\n",
        "\n",
        "                # RE-MAP: Change class 0 to class 4\n",
        "                if int(parts[0]) == POTHOLE_CLASS_ID_ORIGINAL:\n",
        "                    new_label_content.append(f\"{POTHOLE_CLASS_ID_TARGET} {' '.join(parts[1:])}\")\n",
        "                    annos_merged += 1\n",
        "\n",
        "            # If we found potholes, save the new label and copy the image\n",
        "            if new_label_content:\n",
        "                new_img_name = f\"pothole_boost_{i}_{img_path.name}\"\n",
        "\n",
        "                # Write new label file\n",
        "                with open(target_label_dir / (Path(new_img_name).stem + '.txt'), 'w') as f:\n",
        "                    f.write('\\n'.join(new_label_content))\n",
        "\n",
        "                # Copy image\n",
        "                shutil.copy(img_path, target_img_dir / new_img_name)\n",
        "                images_merged += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_path.name}: {e}\")\n",
        "\n",
        "    return images_merged, annos_merged\n",
        "\n",
        "# Run the merge\n",
        "print(\"Merging 'train' split...\")\n",
        "train_imgs, train_annos = merge_and_remap(new_train_images, 'train')\n",
        "print(f\"Merged {train_imgs} new train images with {train_annos} pothole annotations.\")\n",
        "\n",
        "print(\"Merging 'val' split...\")\n",
        "val_imgs, val_annos = merge_and_remap(new_val_images, 'val')\n",
        "print(f\"Merged {val_imgs} new val images with {val_annos} pothole annotations.\")\n",
        "\n",
        "\n",
        "# --- 6. Update the dataset.yaml file ---\n",
        "# We just need to update the 'path' in the YAML we copied\n",
        "boosted_yaml_path = BOOSTED_DATASET_PATH / 'dataset.yaml'\n",
        "if boosted_yaml_path.exists():\n",
        "    with open(boosted_yaml_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    # Update path to be the new absolute path\n",
        "    config['path'] = str(BOOSTED_DATASET_PATH.absolute())\n",
        "\n",
        "    with open(boosted_yaml_path, 'w') as f:\n",
        "        yaml.dump(config, f, sort_keys=False)\n",
        "\n",
        "    # Overwrite the global yaml_file variable\n",
        "    yaml_file = str(boosted_yaml_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ 'Pothole-Boosted' 5-Class dataset created.\")\n",
        "    print(f\"The pipeline will now use this new dataset: {yaml_file}\")\n",
        "else:\n",
        "    print(f\"‚ùå FAILED to find copied dataset.yaml at {boosted_yaml_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7cRgI0xoT3u",
        "outputId": "995f34e8-c096-49c1-dc59-52374dd69f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All classes defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- CLASS NAMES (Must match your YAML) ---\n",
        "CLASS_NAMES = {\n",
        "    0: 'longitudinal crack',\n",
        "    1: 'transverse crack',\n",
        "    2: 'alligator crack',\n",
        "    3: 'other corruption',\n",
        "    4: 'Pothole'\n",
        "}\n",
        "\n",
        "# --- A. GPU RESOURCE MONITOR ---\n",
        "class ColabResourceMonitor:\n",
        "    \"\"\"Monitor resources and set defaults for Colab GPU.\"\"\"\n",
        "\n",
        "    def __init__(self, max_training_hours=2):\n",
        "        self.max_training_hours = max_training_hours\n",
        "        self.start_time = time.time()\n",
        "        self.gpu_available = torch.cuda.is_available()\n",
        "        self.initial_setup()\n",
        "\n",
        "    def initial_setup(self):\n",
        "        print(\"\\nAnalyzing Colab Runtime (Optimized for GPU Performance)...\")\n",
        "        cpu_count = psutil.cpu_count(logical=False) or psutil.cpu_count()\n",
        "\n",
        "        if self.gpu_available:\n",
        "            self.batch_size = 16\n",
        "            self.image_size = 640\n",
        "            self.workers = 8\n",
        "            self.device = None\n",
        "            self.amp = True\n",
        "        else:\n",
        "            print(\"WARNING: GPU not found, falling back to slow CPU settings.\")\n",
        "            self.batch_size = 2\n",
        "            self.image_size = 416\n",
        "            self.workers = 0\n",
        "            self.device = 'cpu'\n",
        "            self.amp = False\n",
        "\n",
        "        print(f\"Optimized settings: Batch Size={self.batch_size}, Image Size={self.image_size}, Workers={self.workers}\")\n",
        "\n",
        "    def get_optimized_config(self):\n",
        "        return {\n",
        "            'batch_size': self.batch_size,\n",
        "            'image_size': self.image_size,\n",
        "            'workers': self.workers,\n",
        "            'epochs': 30, # 30 epochs on a ~4K dataset is a good, fast run\n",
        "            'patience': 10,\n",
        "            'amp': self.amp,\n",
        "            'cache': True, # Cache the dataset in RAM for max speed\n",
        "            'save_period': -1,\n",
        "            'device': self.device\n",
        "        }\n",
        "\n",
        "# --- B. CLASS DISTRIBUTION ANALYZER ---\n",
        "class ClassDistributionAnalyzer:\n",
        "    \"\"\"Analyzes and prints the class distribution in the dataset's train split.\"\"\"\n",
        "    def __init__(self, yaml_path):\n",
        "        self.yaml_path = Path(yaml_path)\n",
        "        self.class_counts = {i: 0 for i in CLASS_NAMES.keys()}\n",
        "\n",
        "    def run_analysis(self):\n",
        "        if not self.yaml_path.exists():\n",
        "            print(\"YAML file not found.\")\n",
        "            return\n",
        "\n",
        "        with open(self.yaml_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "\n",
        "        base_path = Path(config.get('path', self.yaml_path.parent))\n",
        "        train_labels_dir = base_path / config.get('train', 'images/train').replace('images', 'labels')\n",
        "\n",
        "        if not train_labels_dir.is_dir():\n",
        "            print(f\"Training labels directory not found: {train_labels_dir}\")\n",
        "            return\n",
        "\n",
        "        train_label_files = list(train_labels_dir.glob('*.txt'))\n",
        "        total_annotations = 0\n",
        "\n",
        "        for label_file in train_label_files:\n",
        "            try:\n",
        "                with open(label_file, 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    if content:\n",
        "                        for line in content.split('\\n'):\n",
        "                            if line.strip():\n",
        "                                class_id = int(line.split()[0])\n",
        "                                if class_id in self.class_counts:\n",
        "                                    self.class_counts[class_id] += 1\n",
        "                                    total_annotations += 1\n",
        "            except: continue\n",
        "\n",
        "        print(f\"\\nDataset Analysis ({len(train_label_files)} train labels):\")\n",
        "        print(\"------------------------------------------\")\n",
        "        print(f\"Total Annotations Found: {total_annotations}\")\n",
        "        if total_annotations > 0:\n",
        "            for class_id, count in self.class_counts.items():\n",
        "                percentage = (count / total_annotations) * 100\n",
        "                print(f\"  {CLASS_NAMES[class_id]:<20}: {count:>5} ({percentage:.1f}%) \")\n",
        "        else:\n",
        "            print(\"  WARNING: Could not read any annotations. Training may fail.\")\n",
        "        print(\"------------------------------------------\")\n",
        "\n",
        "# --- C. BAYESIAN OPTIMIZER (Definition only) ---\n",
        "class BayesianOptimizer:\n",
        "    def __init__(self, dataset_yaml, resource_monitor):\n",
        "        pass # We are skipping optimization\n",
        "    def optimize(self, n_trials=10, timeout_minutes=15):\n",
        "        pass # We are skipping optimization\n",
        "\n",
        "# --- D. OPTIMIZED TRAINER (GPU, Drive Storage, yolov8s) ---\n",
        "class OptimizedTrainer:\n",
        "\n",
        "    def __init__(self, dataset_yaml, resource_monitor, best_params):\n",
        "        self.dataset_yaml = dataset_yaml\n",
        "        self.monitor = resource_monitor\n",
        "        self.best_params = best_params\n",
        "        self.model = None\n",
        "        self.results = None\n",
        "\n",
        "        # Save to a NEW folder on your Drive\n",
        "        self.PROJECT_PATH = '/content/drive/MyDrive/RDD_GPU_Training_Runs'\n",
        "        self.run_name = 'rdd_2k_boosted_yolov8s_run' # New fixed name\n",
        "\n",
        "        self.MAX_EPOCHS = self.monitor.get_optimized_config()['epochs']\n",
        "        self.PATIENCE = self.monitor.get_optimized_config()['patience']\n",
        "        self.BASE_LINE_MAP = 0.417\n",
        "\n",
        "        self.final_model_path = Path(self.PROJECT_PATH) / self.run_name / 'weights' / 'best.pt'\n",
        "\n",
        "    def train_model(self, resume=False):\n",
        "        print(f\"\\nStarting final training on yolov8s ({self.MAX_EPOCHS} epochs max)...\")\n",
        "\n",
        "        model_path = ''\n",
        "        if resume:\n",
        "            model_path = str(Path(self.PROJECT_PATH) / self.run_name / 'weights' / 'last.pt')\n",
        "            print(f\"Loading model from checkpoint: {model_path}\")\n",
        "        else:\n",
        "            model_path = 'yolov8s.pt'\n",
        "            print(f\"Loading base model: {model_path}\")\n",
        "\n",
        "        self.model = YOLO(model_path)\n",
        "        config = self.monitor.get_optimized_config()\n",
        "\n",
        "        train_params = {\n",
        "            'data': self.dataset_yaml,\n",
        "            'epochs': self.MAX_EPOCHS,\n",
        "            'batch': config['batch_size'],\n",
        "            'imgsz': config['image_size'],\n",
        "            'lr0': self.best_params.get('lr0', 0.01),\n",
        "            'box': self.best_params.get('box', 0.05),\n",
        "            'cls': self.best_params.get('cls', 0.5),\n",
        "            'warmup_epochs': self.best_params.get('warmup_epochs', 5),\n",
        "            'degrees': self.best_params.get('degrees', 10),\n",
        "            'scale': self.best_params.get('scale', 0.3),\n",
        "            'patience': self.PATIENCE,\n",
        "            'workers': config['workers'],\n",
        "            'amp': config['amp'],\n",
        "            'cache': config['cache'],\n",
        "            'save_period': config['save_period'],\n",
        "            'cos_lr': True,\n",
        "            'device': config['device'],\n",
        "            'project': self.PROJECT_PATH,\n",
        "            'name': self.run_name,\n",
        "            'plots': True, 'verbose': True, 'exist_ok': True,\n",
        "            'resume': resume\n",
        "        }\n",
        "\n",
        "        print(f\"Final Config: Epochs={self.MAX_EPOCHS}, Batch={config['batch_size']}, Device={config['device']}\")\n",
        "        print(f\"Checkpoints will be saved to: {self.PROJECT_PATH}/{self.run_name}\")\n",
        "\n",
        "        try:\n",
        "            self.results = self.model.train(**train_params)\n",
        "            print(f\"\\n‚úÖ Training complete. Best model saved to: {self.final_model_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        print(\"\\n==================================================\")\n",
        "        print(\"DETAILED BOOSTED MODEL EVALUATION\")\n",
        "        print(\"==================================================\")\n",
        "\n",
        "        if not self.final_model_path.exists():\n",
        "            print(f\"‚ùå Error: Model not found at {self.final_model_path}.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Loading best model from: {self.final_model_path}\")\n",
        "        model = YOLO(self.final_model_path)\n",
        "\n",
        "        print(\"Running validation to generate metrics...\")\n",
        "        val_results = model.val(data=self.dataset_yaml, verbose=True)\n",
        "\n",
        "        performance = {\n",
        "            'mAP50': val_results.box.map50,\n",
        "            'mAP50_95': val_results.box.map,\n",
        "            'precision': val_results.box.mp,\n",
        "            'recall': val_results.box.mr,\n",
        "        }\n",
        "\n",
        "        print(f\"\\nPerformance Metrics (Validation Set):\")\n",
        "        print(f\"  mAP@0.5:    {performance['mAP50']:.4f} ({performance['mAP50']:.1%}) <--- Main Score\")\n",
        "        print(f\"  mAP@0.5:0.95: {performance['mAP50_95']:.4f} ({performance['mAP50_95']:.1%})\")\n",
        "        print(f\"  Precision:  {performance['precision']:.4f} ({performance['precision']:.1%})\")\n",
        "        print(f\"  Recall:     {performance['recall']:.4f} ({performance['recall']:.1%})\")\n",
        "\n",
        "        print(\n",
        "            \"\\nPer-Class mAP@0.5:\\n\" +\n",
        "            \"----------------------\")\n",
        "        class_results = val_results.box.ap_class_index\n",
        "        class_maps = val_results.box.ap\n",
        "\n",
        "        for i, class_index in enumerate(class_results):\n",
        "            class_name = CLASS_NAMES.get(class_index, f'Class {class_index}')\n",
        "            map_value = class_maps[i] if np.isfinite(class_maps[i]) else 0.0\n",
        "            print(f\"  - {class_name:<20}: {map_value:.4f}\")\n",
        "\n",
        "        improvement = performance['mAP50'] - self.BASE_LINE_MAP\n",
        "        print(f\"\\nImprovement vs RDD Baseline ({self.BASE_LINE_MAP:.1%}): {improvement:+.1%}\")\n",
        "\n",
        "        return performance\n",
        "\n",
        "    def download_best_model(self):\n",
        "        print(\"\\n==================================================\")\n",
        "        print(\"DOWNLOAD MODEL\")\n",
        "        print(\"==================================================\")\n",
        "        if not self.final_model_path.exists():\n",
        "            print(f\"‚ùå Error: Model not found at {self.final_model_path}.\")\n",
        "            return\n",
        "        print(f\"Preparing to download: {self.final_model_path}\")\n",
        "        files.download(str(self.final_model_path))\n",
        "\n",
        "print(\"All classes defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC8qooTmoVfc",
        "outputId": "dc8664f3-e864-426f-a100-a344a7c7dc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing Colab Runtime (Optimized for GPU Performance)...\n",
            "Optimized settings: Batch Size=16, Image Size=640, Workers=8\n",
            "\n",
            "Dataset Analysis (3200 train labels):\n",
            "------------------------------------------\n",
            "Total Annotations Found: 8546\n",
            "  longitudinal crack  :   910 (10.6%) \n",
            "  transverse crack    :   874 (10.2%) \n",
            "  alligator crack     :   849 (9.9%) \n",
            "  other corruption    :   842 (9.9%) \n",
            "  Pothole             :  5071 (59.3%) \n",
            "------------------------------------------\n",
            "‚úÖ RECOVERY: Loaded best_params from previous run.\n",
            "Trainer created. Ready to train on: /content/drive/MyDrive/RDD_2K_Pothole_Boosted/dataset.yaml\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 4: Initialization and Recovery ---\n",
        "\n",
        "# 1. Initialize resource monitor (sets GPU-safe parameters)\n",
        "monitor = ColabResourceMonitor(max_training_hours=2)\n",
        "\n",
        "# 2. Analyze and print the class distribution of the NEW BOOSTED dataset\n",
        "# You should see a much higher number for 'Pothole' now\n",
        "analyzer = ClassDistributionAnalyzer(yaml_file)\n",
        "analyzer.run_analysis()\n",
        "\n",
        "# 3. Load Best Params (Recovery)\n",
        "# We are skipping the 20-min optimization and re-using your previous results.\n",
        "best_params = {\n",
        "    'lr0': 0.018415672546309683,\n",
        "    'box': 0.05035344914171263,\n",
        "    'cls': 0.3397850106139112,\n",
        "    'warmup_epochs': 3,\n",
        "    'degrees': 8.046959850463304,\n",
        "    'scale': 0.26962935492158147\n",
        "}\n",
        "print(\"‚úÖ RECOVERY: Loaded best_params from previous run.\")\n",
        "\n",
        "# 4. Create the Trainer Object\n",
        "if 'yaml_file' in globals():\n",
        "    trainer = OptimizedTrainer(yaml_file, monitor, best_params)\n",
        "    print(f\"Trainer created. Ready to train on: {yaml_file}\")\n",
        "else:\n",
        "    print(\"‚ùå 'yaml_file' not defined. Please run Cell 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9S6rTdeoaMu",
        "outputId": "3d595e86-4a89-45ca-ea80-788d77ea24ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è No checkpoint found. Starting a new training run.\n",
            "(Progress will be saved to /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run)\n",
            "\n",
            "Starting final training on yolov8s (30 epochs max)...\n",
            "Loading base model: yolov8s.pt\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.5MB 187.8MB/s 0.1s\n",
            "Final Config: Epochs=30, Batch=16, Device=None\n",
            "Checkpoints will be saved to: /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run\n",
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=0.05035344914171263, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.3397850106139112, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/content/drive/MyDrive/RDD_2K_Pothole_Boosted/dataset.yaml, degrees=8.046959850463304, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.018415672546309683, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=rdd_2k_boosted_yolov8s_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/RDD_GPU_Training_Runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.26962935492158147, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 26.0MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,137,535 parameters, 11,137,519 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 101.7MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 1.3¬±1.6 ms, read: 31.7¬±41.2 MB/s, size: 239.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/RDD_2K_Pothole_Boosted/labels/train... 3200 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3200/3200 159.9it/s 20.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/RDD_2K_Pothole_Boosted/images/train/Japan_006916.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/RDD_2K_Pothole_Boosted/labels/train.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 307, len(boxes) = 8545. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "WARNING ‚ö†Ô∏è cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (3.6GB RAM): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3200/3200 90.3it/s 35.4s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 4.3¬±7.7 ms, read: 19.7¬±23.1 MB/s, size: 208.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/RDD_2K_Pothole_Boosted/labels/val... 7733 images, 1840 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7736/7736 127.7it/s 1:01\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/RDD_2K_Pothole_Boosted/images/val/Japan_006536.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/RDD_2K_Pothole_Boosted/labels/val.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 53, len(boxes) = 14971. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mval: \u001b[0m12.4GB RAM required to cache images with 50% safety margin but only 5.6/12.7GB available, not caching images\n",
            "Plotting labels to /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.018415672546309683' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      3.73G    0.01346      2.027      1.931         57        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.8it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:28\n",
            "                   all       7736      14971      0.293      0.189      0.141     0.0494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      4.88G    0.01325      1.683      1.978         78        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.198      0.246      0.135     0.0505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      4.92G    0.01327      1.634      2.015         85        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:23\n",
            "                   all       7736      14971      0.252      0.228      0.163      0.059\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      4.92G    0.01316      1.607      2.006         64        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:22\n",
            "                   all       7736      14971      0.268      0.245      0.171     0.0606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      4.92G    0.01289      1.569      1.962         67        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:19\n",
            "                   all       7736      14971       0.29      0.274      0.207       0.08\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      4.92G    0.01261       1.49      1.954         64        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:24\n",
            "                   all       7736      14971      0.322      0.268      0.232     0.0875\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      4.92G    0.01242      1.457      1.914         61        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:22\n",
            "                   all       7736      14971      0.392      0.288      0.249     0.0947\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      4.92G    0.01222       1.39      1.889         42        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.345      0.313      0.258      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      4.92G    0.01209      1.367      1.886         52        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.356      0.328      0.278      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      4.92G    0.01198      1.341       1.87         70        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:23\n",
            "                   all       7736      14971      0.385      0.327      0.292      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      4.92G    0.01189      1.314      1.865         67        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.393      0.366      0.321      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      4.92G    0.01173      1.269      1.829         72        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:23\n",
            "                   all       7736      14971       0.37      0.337      0.289      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      4.92G    0.01148      1.231      1.811         52        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:23\n",
            "                   all       7736      14971      0.424      0.393      0.345      0.146\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      4.92G     0.0113       1.19      1.785         44        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:26\n",
            "                   all       7736      14971      0.416      0.374      0.337       0.14\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      4.95G    0.01123      1.176      1.776         53        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:29\n",
            "                   all       7736      14971      0.456      0.394      0.368      0.153\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      4.95G    0.01121      1.159      1.782         72        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:29\n",
            "                   all       7736      14971      0.471      0.405      0.383      0.161\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      4.95G    0.01105      1.113      1.753         54        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:09\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:25\n",
            "                   all       7736      14971      0.449      0.405      0.382      0.162\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      4.95G    0.01087      1.088      1.739         54        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:25\n",
            "                   all       7736      14971      0.474       0.41      0.386       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      4.95G    0.01084      1.067      1.737         41        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:09\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.491      0.425      0.406      0.173\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      4.95G    0.01066      1.039      1.711         55        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 2.9it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.474      0.432      0.405      0.175\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      4.95G     0.0104     0.9043      1.684         35        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:20\n",
            "                   all       7736      14971      0.478       0.43      0.414      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      4.95G    0.01029      0.857      1.669         38        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:23\n",
            "                   all       7736      14971      0.512      0.428      0.421      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      4.95G   0.009975     0.8163       1.64         38        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:21\n",
            "                   all       7736      14971      0.523      0.442      0.432      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      4.95G    0.00989     0.7812      1.623         50        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:22\n",
            "                   all       7736      14971      0.523      0.446      0.429      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      4.95G   0.009756     0.7675        1.6         47        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.0it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:20\n",
            "                   all       7736      14971      0.521      0.454      0.437      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      4.95G   0.009633     0.7402      1.585         49        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:19\n",
            "                   all       7736      14971      0.519      0.453      0.439        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      4.95G   0.009469     0.7243      1.562         40        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:20\n",
            "                   all       7736      14971      0.536      0.449      0.447      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      4.95G   0.009458     0.7099      1.563         50        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:22\n",
            "                   all       7736      14971      0.534      0.451      0.445      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      4.95G   0.009393     0.7014      1.552         52        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:20\n",
            "                   all       7736      14971      0.537      0.454      0.448      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      4.95G   0.009356      0.698      1.551         41        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 200/200 3.1it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.2it/s 3:20\n",
            "                   all       7736      14971       0.53      0.451      0.445      0.202\n",
            "\n",
            "30 epochs completed in 2.277 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt...\n",
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242/242 1.1it/s 3:46\n",
            "                   all       7736      14971      0.536      0.449      0.447      0.204\n",
            "    longitudinal crack       2011       3890       0.54      0.258      0.298      0.119\n",
            "      transverse crack       1158       1769      0.453      0.304      0.292     0.0892\n",
            "       alligator crack       1222       1553      0.459      0.487      0.447      0.198\n",
            "      other corruption       1093       1563       0.53      0.596       0.57      0.286\n",
            "               Pothole       2519       6196      0.697      0.601      0.631      0.329\n",
            "Speed: 0.2ms preprocess, 4.0ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run\u001b[0m\n",
            "\n",
            "‚úÖ Training complete. Best model saved to: /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt\n",
            "\n",
            "‚úÖ Training is complete.\n",
            "Your 'Pothole Boosted' model is saved to Google Drive at: /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt\n",
            "You can now run Cell 6 to evaluate or Cell 7 to download.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 5: Train Boosted Model ---\n",
        "\n",
        "# Define the permanent paths on your Google Drive\n",
        "PERSISTENT_PROJECT_PATH = '/content/drive/MyDrive/RDD_GPU_Training_Runs'\n",
        "PERSISTENT_RUN_NAME = 'rdd_2k_boosted_yolov8s_run' # The new fixed name\n",
        "\n",
        "# Check if a checkpoint ('last.pt') already exists ON GOOGLE DRIVE\n",
        "resume_path = Path(PERSISTENT_PROJECT_PATH) / PERSISTENT_RUN_NAME / 'weights' / 'last.pt'\n",
        "resume_training = resume_path.exists()\n",
        "\n",
        "if resume_training:\n",
        "    print(f\"‚úÖ Checkpoint found at {resume_path}.\")\n",
        "    print(\"Training will resume from your previous progress.\")\n",
        "else:\n",
        "    print(f\"‚ÑπÔ∏è No checkpoint found. Starting a new training run.\")\n",
        "    print(f\"(Progress will be saved to {resume_path.parent.parent})\")\n",
        "\n",
        "\n",
        "if 'trainer' in locals() and Path(yaml_file).exists():\n",
        "\n",
        "    # Pass the 'resume_training' flag to the train_model method\n",
        "    if trainer.train_model(resume=resume_training):\n",
        "        print(\"\\n‚úÖ Training is complete.\")\n",
        "        print(f\"Your 'Pothole Boosted' model is saved to Google Drive at: {trainer.final_model_path}\")\n",
        "        print(\"You can now run Cell 6 to evaluate or Cell 7 to download.\")\n",
        "    else:\n",
        "        print(\"‚ùå Training process failed.\")\n",
        "else:\n",
        "    print(\"Cannot start training: 'trainer' object not defined. Please run Cell 4.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMR-tN71ofcE",
        "outputId": "dd57a92c-e6d4-4551-d333-189afdb77e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DETAILED BOOSTED MODEL EVALUATION\n",
            "==================================================\n",
            "Loading best model from: /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt\n",
            "Running validation to generate metrics...\n",
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.4¬±0.1 ms, read: 7.7¬±4.6 MB/s, size: 61.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/RDD_2K_Pothole_Boosted/labels/val.cache... 7733 images, 1840 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7736/7736 9.8Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/RDD_2K_Pothole_Boosted/images/val/Japan_006536.jpg: 1 duplicate labels removed\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 53, len(boxes) = 14971. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 484/484 2.0it/s 4:01\n",
            "                   all       7736      14971      0.534      0.449      0.448      0.204\n",
            "    longitudinal crack       2011       3890       0.54      0.258      0.298      0.119\n",
            "      transverse crack       1158       1769       0.45      0.303      0.292     0.0892\n",
            "       alligator crack       1222       1553      0.456      0.487      0.447      0.198\n",
            "      other corruption       1093       1563      0.529      0.597       0.57      0.286\n",
            "               Pothole       2519       6196      0.695      0.602      0.631      0.329\n",
            "Speed: 1.0ms preprocess, 8.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "\n",
            "Performance Metrics (Validation Set):\n",
            "  mAP@0.5:    0.4476 (44.8%) <--- Main Score\n",
            "  mAP@0.5:0.95: 0.2042 (20.4%)\n",
            "  Precision:  0.5340 (53.4%)\n",
            "  Recall:     0.4492 (44.9%)\n",
            "\n",
            "Per-Class mAP@0.5:\n",
            "----------------------\n",
            "  - longitudinal crack  : 0.1193\n",
            "  - transverse crack    : 0.0892\n",
            "  - alligator crack     : 0.1978\n",
            "  - other corruption    : 0.2856\n",
            "  - Pothole             : 0.3290\n",
            "\n",
            "Improvement vs RDD Baseline (41.7%): +3.1%\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 6: Evaluate Boosted Model ---\n",
        "\n",
        "if 'trainer' in locals() and trainer.final_model_path.exists():\n",
        "    trainer.evaluate_model()\n",
        "else:\n",
        "    print(\"‚ùå Model not found. Please run Cell 5 to train the model first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "UNFl6qRLogUR",
        "outputId": "639f406e-f566-42ef-86ca-bb8628d1eec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DOWNLOAD MODEL\n",
            "==================================================\n",
            "Preparing to download: /content/drive/MyDrive/RDD_GPU_Training_Runs/rdd_2k_boosted_yolov8s_run/weights/best.pt\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e5d449d8-bc5a-433f-9751-066534f4549f\", \"best.pt\", 22518570)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Cell 7: Download Boosted Model ---\n",
        "\n",
        "if 'trainer' in locals() and trainer.final_model_path.exists():\n",
        "    trainer.download_best_model()\n",
        "else:\n",
        "    print(\"‚ùå Model not found. Please run Cell 5 to train the model first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBXFPPaFUhXZ",
        "outputId": "58394223-4308-484f-96c1-74dab4d6f359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ kaggle.json is installed and ready.\n"
          ]
        }
      ],
      "source": [
        "# --- NEW CELL: Run this after uploading kaggle.json ---\n",
        "\n",
        "import os\n",
        "\n",
        "# Create the .kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move the uploaded kaggle.json file\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the correct permissions (this is a required step)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"‚úÖ kaggle.json is installed and ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data from your Table 5.2 (Simulated Results)\n",
        "# Adjust these values if your simulation produced different numbers\n",
        "labels = ['Severe Repairs', 'Moderate Repairs', 'Minor Repairs (Unfunded)']\n",
        "# Example: 150k + 120k = 270k (Severe), 80k + 90k + 50k = 220k (Moderate), 0 (Minor)\n",
        "# Let's use percentages or raw amounts. Here using raw amounts for the funded parts.\n",
        "# Severe: 270,000\n",
        "# Moderate: 220,000\n",
        "# Remaining/Unfunded: (Representing the gap or just showing funded distribution)\n",
        "\n",
        "# Let's stick to the Funded distribution for clarity as per your description\n",
        "# \"Severe repairs accounted for approximately 55%... Moderate utilized remaining 45%\"\n",
        "sizes = [55, 45] \n",
        "labels_funded = ['Severe Repairs', 'Moderate Repairs']\n",
        "colors = ['#ff9999', '#66b3ff'] # Red-ish for Severe, Blue-ish for Moderate\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sizes, labels=labels_funded, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\n",
        "plt.title('Budget Allocation by Severity Severity')\n",
        "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "# Save the chart\n",
        "plt.savefig('budget_allocation_pie_chart.png')\n",
        "print(\"Pie chart saved as 'budget_allocation_pie_chart.png'\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
